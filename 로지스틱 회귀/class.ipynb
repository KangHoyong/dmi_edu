{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스로 파이토치 모델 구현  : 로지스틱 회귀 \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# [1] x 데이터 생성 , y 정답지 데이터 생성 \n",
    "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]\n",
    "\n",
    "# [2] 만든 x,y 데이터를 텐서로 변경 \n",
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class 생성 class Name = BinaryClassifier(nn.Module)\n",
    "# [3] 모델 클래스 생성 \n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 본인 들이 직접 해보기 code 추가\n",
    "        self.linear = nn.Linear(2,1) # input_dim = 2 output_dim = 1 \n",
    "        self.sigmoid = nn.Sigmoid() # 출력은 시그모이드 함수를 거친다.\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.linear(x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch :    0/1000 loss : 0.614994 Acc 0.67%\nEpoch :   10/1000 loss : 0.559069 Acc 0.83%\nEpoch :   20/1000 loss : 0.541587 Acc 0.83%\nEpoch :   30/1000 loss : 0.526862 Acc 0.83%\nEpoch :   40/1000 loss : 0.514000 Acc 0.83%\nEpoch :   50/1000 loss : 0.502443 Acc 0.83%\nEpoch :   60/1000 loss : 0.491833 Acc 0.83%\nEpoch :   70/1000 loss : 0.481934 Acc 0.83%\nEpoch :   80/1000 loss : 0.472587 Acc 0.83%\nEpoch :   90/1000 loss : 0.463683 Acc 0.83%\nEpoch :  100/1000 loss : 0.455148 Acc 0.83%\nEpoch :  110/1000 loss : 0.446927 Acc 0.83%\nEpoch :  120/1000 loss : 0.438981 Acc 0.83%\nEpoch :  130/1000 loss : 0.431283 Acc 0.83%\nEpoch :  140/1000 loss : 0.423811 Acc 0.83%\nEpoch :  150/1000 loss : 0.416548 Acc 0.83%\nEpoch :  160/1000 loss : 0.409482 Acc 0.83%\nEpoch :  170/1000 loss : 0.402603 Acc 0.83%\nEpoch :  180/1000 loss : 0.395903 Acc 0.83%\nEpoch :  190/1000 loss : 0.389375 Acc 0.83%\nEpoch :  200/1000 loss : 0.383012 Acc 0.83%\nEpoch :  210/1000 loss : 0.376810 Acc 0.83%\nEpoch :  220/1000 loss : 0.370763 Acc 0.83%\nEpoch :  230/1000 loss : 0.364867 Acc 0.83%\nEpoch :  240/1000 loss : 0.359117 Acc 0.83%\nEpoch :  250/1000 loss : 0.353511 Acc 0.83%\nEpoch :  260/1000 loss : 0.348043 Acc 0.83%\nEpoch :  270/1000 loss : 0.342710 Acc 0.83%\nEpoch :  280/1000 loss : 0.337508 Acc 0.83%\nEpoch :  290/1000 loss : 0.332434 Acc 0.83%\nEpoch :  300/1000 loss : 0.327485 Acc 0.83%\nEpoch :  310/1000 loss : 0.322657 Acc 0.83%\nEpoch :  320/1000 loss : 0.317946 Acc 0.83%\nEpoch :  330/1000 loss : 0.313350 Acc 0.83%\nEpoch :  340/1000 loss : 0.308865 Acc 0.83%\nEpoch :  350/1000 loss : 0.304488 Acc 0.83%\nEpoch :  360/1000 loss : 0.300217 Acc 0.83%\nEpoch :  370/1000 loss : 0.296048 Acc 0.83%\nEpoch :  380/1000 loss : 0.291978 Acc 0.83%\nEpoch :  390/1000 loss : 0.288004 Acc 0.83%\nEpoch :  400/1000 loss : 0.284125 Acc 0.83%\nEpoch :  410/1000 loss : 0.280336 Acc 0.83%\nEpoch :  420/1000 loss : 0.276636 Acc 0.83%\nEpoch :  430/1000 loss : 0.273022 Acc 0.83%\nEpoch :  440/1000 loss : 0.269491 Acc 0.83%\nEpoch :  450/1000 loss : 0.266041 Acc 0.83%\nEpoch :  460/1000 loss : 0.262670 Acc 0.83%\nEpoch :  470/1000 loss : 0.259376 Acc 0.83%\nEpoch :  480/1000 loss : 0.256156 Acc 0.83%\nEpoch :  490/1000 loss : 0.253008 Acc 1.00%\nEpoch :  500/1000 loss : 0.249930 Acc 1.00%\nEpoch :  510/1000 loss : 0.246920 Acc 1.00%\nEpoch :  520/1000 loss : 0.243976 Acc 1.00%\nEpoch :  530/1000 loss : 0.241097 Acc 1.00%\nEpoch :  540/1000 loss : 0.238280 Acc 1.00%\nEpoch :  550/1000 loss : 0.235523 Acc 1.00%\nEpoch :  560/1000 loss : 0.232825 Acc 1.00%\nEpoch :  570/1000 loss : 0.230185 Acc 1.00%\nEpoch :  580/1000 loss : 0.227600 Acc 1.00%\nEpoch :  590/1000 loss : 0.225070 Acc 1.00%\nEpoch :  600/1000 loss : 0.222592 Acc 1.00%\nEpoch :  610/1000 loss : 0.220165 Acc 1.00%\nEpoch :  620/1000 loss : 0.217788 Acc 1.00%\nEpoch :  630/1000 loss : 0.215459 Acc 1.00%\nEpoch :  640/1000 loss : 0.213177 Acc 1.00%\nEpoch :  650/1000 loss : 0.210941 Acc 1.00%\nEpoch :  660/1000 loss : 0.208749 Acc 1.00%\nEpoch :  670/1000 loss : 0.206601 Acc 1.00%\nEpoch :  680/1000 loss : 0.204494 Acc 1.00%\nEpoch :  690/1000 loss : 0.202429 Acc 1.00%\nEpoch :  700/1000 loss : 0.200403 Acc 1.00%\nEpoch :  710/1000 loss : 0.198417 Acc 1.00%\nEpoch :  720/1000 loss : 0.196468 Acc 1.00%\nEpoch :  730/1000 loss : 0.194556 Acc 1.00%\nEpoch :  740/1000 loss : 0.192679 Acc 1.00%\nEpoch :  750/1000 loss : 0.190838 Acc 1.00%\nEpoch :  760/1000 loss : 0.189030 Acc 1.00%\nEpoch :  770/1000 loss : 0.187256 Acc 1.00%\nEpoch :  780/1000 loss : 0.185514 Acc 1.00%\nEpoch :  790/1000 loss : 0.183803 Acc 1.00%\nEpoch :  800/1000 loss : 0.182123 Acc 1.00%\nEpoch :  810/1000 loss : 0.180473 Acc 1.00%\nEpoch :  820/1000 loss : 0.178852 Acc 1.00%\nEpoch :  830/1000 loss : 0.177260 Acc 1.00%\nEpoch :  840/1000 loss : 0.175695 Acc 1.00%\nEpoch :  850/1000 loss : 0.174157 Acc 1.00%\nEpoch :  860/1000 loss : 0.172646 Acc 1.00%\nEpoch :  870/1000 loss : 0.171160 Acc 1.00%\nEpoch :  880/1000 loss : 0.169699 Acc 1.00%\nEpoch :  890/1000 loss : 0.168263 Acc 1.00%\nEpoch :  900/1000 loss : 0.166851 Acc 1.00%\nEpoch :  910/1000 loss : 0.165462 Acc 1.00%\nEpoch :  920/1000 loss : 0.164096 Acc 1.00%\nEpoch :  930/1000 loss : 0.162752 Acc 1.00%\nEpoch :  940/1000 loss : 0.161430 Acc 1.00%\nEpoch :  950/1000 loss : 0.160130 Acc 1.00%\nEpoch :  960/1000 loss : 0.158849 Acc 1.00%\nEpoch :  970/1000 loss : 0.157590 Acc 1.00%\nEpoch :  980/1000 loss : 0.156350 Acc 1.00%\nEpoch :  990/1000 loss : 0.155129 Acc 1.00%\nEpoch : 1000/1000 loss : 0.153927 Acc 1.00%\n"
    }
   ],
   "source": [
    "# [3] 모델 선언\n",
    "model = BinaryClassifier()\n",
    "\n",
    "# optimizer 설정 \n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# 얼마 만큼 반복하면서 Train 할것인가 ?!\n",
    "epochs_num = 1000\n",
    "\n",
    "for epoch in range(epochs_num + 1):\n",
    "\n",
    "    # H(x) 계산 \n",
    "    hypothesis = model(x_train)\n",
    "\n",
    "    # loss \n",
    "    loss = F.binary_cross_entropy(hypothesis, y_train)\n",
    "\n",
    "    # loss H(x) 개선 \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 20번마다 로그 출력 \n",
    "    if epoch % 10 == 0 : \n",
    "        prediction = hypothesis >= torch.FloatTensor([0.5]) # 0.5 보다 크면 True로 간주 \n",
    "        correct_prediction = prediction.float() == y_train # 실제값과 일치하면 True로 간주 \n",
    "        acc = correct_prediction.sum().item() / len(correct_prediction) #  정확도 계산 \n",
    "        print(\"Epoch : {:4d}/{} loss : {:.6f} Acc {:2.2f}%\".format(epoch, epochs_num, loss, acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[0.0326],\n        [0.1613],\n        [0.3136],\n        [0.7774],\n        [0.9371],\n        [0.9793]], grad_fn=<SigmoidBackward>)"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# 모델 학습이 정확하게 되었는지 안되었는지 테스트 \n",
    "model(x_train)\n",
    "# 정답지 : y_data = [[0], [0], [0], [1], [1], [1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('dp': venv)",
   "language": "python",
   "name": "python37764bitdpvenv3835be669c484e1397b1b7ff62365c4c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}