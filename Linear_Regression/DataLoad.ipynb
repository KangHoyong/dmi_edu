{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/200 Batch 1/3 loss: 63938.742188\n",
      "Epoch    0/200 Batch 2/3 loss: 9781.202148\n",
      "Epoch    0/200 Batch 3/3 loss: 3319.261963\n",
      "Epoch   10/200 Batch 1/3 loss: 0.091050\n",
      "Epoch   10/200 Batch 2/3 loss: 0.672307\n",
      "Epoch   10/200 Batch 3/3 loss: 0.269770\n",
      "Epoch   20/200 Batch 1/3 loss: 0.255451\n",
      "Epoch   20/200 Batch 2/3 loss: 0.252995\n",
      "Epoch   20/200 Batch 3/3 loss: 0.258277\n",
      "Epoch   30/200 Batch 1/3 loss: 0.390200\n",
      "Epoch   30/200 Batch 2/3 loss: 0.124120\n",
      "Epoch   30/200 Batch 3/3 loss: 0.299874\n",
      "Epoch   40/200 Batch 1/3 loss: 0.181909\n",
      "Epoch   40/200 Batch 2/3 loss: 0.369281\n",
      "Epoch   40/200 Batch 3/3 loss: 0.165709\n",
      "Epoch   50/200 Batch 1/3 loss: 0.097081\n",
      "Epoch   50/200 Batch 2/3 loss: 0.439587\n",
      "Epoch   50/200 Batch 3/3 loss: 0.402269\n",
      "Epoch   60/200 Batch 1/3 loss: 0.339854\n",
      "Epoch   60/200 Batch 2/3 loss: 0.236968\n",
      "Epoch   60/200 Batch 3/3 loss: 0.277400\n",
      "Epoch   70/200 Batch 1/3 loss: 0.352972\n",
      "Epoch   70/200 Batch 2/3 loss: 0.167230\n",
      "Epoch   70/200 Batch 3/3 loss: 0.474567\n",
      "Epoch   80/200 Batch 1/3 loss: 0.179132\n",
      "Epoch   80/200 Batch 2/3 loss: 0.324070\n",
      "Epoch   80/200 Batch 3/3 loss: 0.314658\n",
      "Epoch   90/200 Batch 1/3 loss: 0.362829\n",
      "Epoch   90/200 Batch 2/3 loss: 0.122568\n",
      "Epoch   90/200 Batch 3/3 loss: 0.227985\n",
      "Epoch  100/200 Batch 1/3 loss: 0.143057\n",
      "Epoch  100/200 Batch 2/3 loss: 0.422572\n",
      "Epoch  100/200 Batch 3/3 loss: 0.187422\n",
      "Epoch  110/200 Batch 1/3 loss: 0.356181\n",
      "Epoch  110/200 Batch 2/3 loss: 0.187673\n",
      "Epoch  110/200 Batch 3/3 loss: 0.337625\n",
      "Epoch  120/200 Batch 1/3 loss: 0.096666\n",
      "Epoch  120/200 Batch 2/3 loss: 0.648426\n",
      "Epoch  120/200 Batch 3/3 loss: 0.241783\n",
      "Epoch  130/200 Batch 1/3 loss: 0.247584\n",
      "Epoch  130/200 Batch 2/3 loss: 0.255172\n",
      "Epoch  130/200 Batch 3/3 loss: 0.230572\n",
      "Epoch  140/200 Batch 1/3 loss: 0.104563\n",
      "Epoch  140/200 Batch 2/3 loss: 0.633505\n",
      "Epoch  140/200 Batch 3/3 loss: 0.239552\n",
      "Epoch  150/200 Batch 1/3 loss: 0.068457\n",
      "Epoch  150/200 Batch 2/3 loss: 0.643451\n",
      "Epoch  150/200 Batch 3/3 loss: 0.140088\n",
      "Epoch  160/200 Batch 1/3 loss: 0.432632\n",
      "Epoch  160/200 Batch 2/3 loss: 0.192002\n",
      "Epoch  160/200 Batch 3/3 loss: 0.112547\n",
      "Epoch  170/200 Batch 1/3 loss: 0.418518\n",
      "Epoch  170/200 Batch 2/3 loss: 0.140293\n",
      "Epoch  170/200 Batch 3/3 loss: 0.134782\n",
      "Epoch  180/200 Batch 1/3 loss: 0.052377\n",
      "Epoch  180/200 Batch 2/3 loss: 0.440659\n",
      "Epoch  180/200 Batch 3/3 loss: 0.219040\n",
      "Epoch  190/200 Batch 1/3 loss: 0.529058\n",
      "Epoch  190/200 Batch 2/3 loss: 0.338697\n",
      "Epoch  190/200 Batch 3/3 loss: 0.087087\n",
      "Epoch  200/200 Batch 1/3 loss: 0.049803\n",
      "Epoch  200/200 Batch 2/3 loss: 0.482526\n",
      "Epoch  200/200 Batch 3/3 loss: 0.234028\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# TensorDataset and DataLoader \n",
    "from torch.utils.data import TensorDataset # 텐서 데이터셋\n",
    "from torch.utils.data import DataLoader # 데이터로더\n",
    "\n",
    "# TensorDataset은 기본적으로 텐서를 입력으로 받습니다. 텐서 형태로 데이터를 정의합니다.\n",
    "x_train  =  torch.FloatTensor([[73,  80,  75], \n",
    "                               [93,  88,  93], \n",
    "                               [89,  91,  90], \n",
    "                               [96,  98,  100],   \n",
    "                               [73,  66,  70]])  \n",
    "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])\n",
    "\n",
    "# TensorDataset의 입력으로 사용하고 dataset으로 저장합니다.\n",
    "dataset = TensorDataset(x_train, y_train)\n",
    "\"\"\"\n",
    "데이터로더는 기본적으로 2개의 인자를 입력받는다. 하나는 데이터셋, 미니 배치의 크기입니다. \n",
    "이때 미니 배치의 크기는 통상적으로 2의 배수를 사용합니다. (ex) 64, 128, 256...) 그리고 추가적으로 많이 사용되는 인자로 shuffle이 있습니다. \n",
    "shuffle=True를 선택하면 Epoch마다 데이터셋을 섞어서 데이터가 학습되는 순서를 바꿉니다.\n",
    "\n",
    "사람도 같은 문제지를 계속 풀면 어느 순간 문제의 순서에 익숙해질 수 있습니다. \n",
    "예를 들어 어떤 문제지의 12번 문제를 풀면서, '13번 문제가 뭔지는 기억은 안 나지만 어제 풀었던 기억으로 정답은 5번이었던 것 같은데' \n",
    "하면서 문제 자체보단 순서에 익숙해질 수 있다는 것입니다. 그럴 때 문제지를 풀 때마다 문제 순서를 랜덤으로 바꾸면 도움이 될 겁니다. \n",
    "마찬가지로 모델이 데이터셋의 순서에 익숙해지는 것을 방지하여 학습할 때는 이 옵션을 True를 주는 것을 권장합니다.\n",
    "\"\"\"\n",
    "\n",
    "# dataloader \n",
    "dataloader = DataLoader(dataset, batch_size = 2 , shuffle=True)\n",
    "\n",
    "# 이제 모델과 옵티마이저 설계 \n",
    "model = nn.Linear(3,1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)\n",
    "\n",
    "epochs_nb = 200\n",
    "for epoch in range(epochs_nb + 1):\n",
    "    for batch_idx , samples in enumerate(dataloader):\n",
    "#         print(batch_idx)\n",
    "#         print(samples)\n",
    "        \n",
    "        x_train , y_train = samples\n",
    "        \n",
    "        # H(x) 계산\n",
    "        prediction = model(x_train)\n",
    "        \n",
    "        # loss \n",
    "        loss = F.mse_loss(prediction, y_train)\n",
    "        \n",
    "        # cost H(x) 계산 \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 10 == 0 : \n",
    "            \n",
    "            print('Epoch {:4d}/{} Batch {}/{} loss: {:.6f}'.format(epoch, epochs_nb, batch_idx+1, len(dataloader),\n",
    "            loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 후 입력이 73 80 75일 때 예측값 :  tensor([[151.5240]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 모델의 입력으로 임의의 값을 넣어 예측값을 확인\n",
    "\n",
    "# 임의의 입력 값 \n",
    "test_val = torch.FloatTensor([[73, 80 , 75]])\n",
    "\n",
    "# 입력한 값 [73, 80 , 75]에 대해서 예측값 y를 리턴받아서 pred_y에 저장 \n",
    "pred_y = model(test_val)\n",
    "print(\"훈련 후 입력이 73 80 75일 때 예측값 : \", pred_y)\n",
    "\n",
    "# 정답지 : 73 80 75 -> y 값이 152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('dp': venv)",
   "language": "python",
   "name": "python37764bitdpvenv3835be669c484e1397b1b7ff62365c4c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
