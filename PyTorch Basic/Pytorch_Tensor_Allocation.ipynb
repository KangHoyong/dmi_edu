{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1D with pyTorch test print :  tensor([0., 1., 2., 3., 4., 5., 6.])\n"
     ]
    }
   ],
   "source": [
    "# 파이토치 텐서 선언하기 \n",
    "\n",
    "# 파이토치는 Numpy와 매우 유사합니다 \n",
    "\n",
    "import torch\n",
    "# 1. 1D with pyTorch\n",
    "# 1차원 백터를 선언 \n",
    "t_torch = torch.FloatTensor([0., 1., 2., 3., 4., 5., 6.])\n",
    "print(\"1D with pyTorch test print : \", t_torch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyTorch 텐서 차원 info\n",
      "1\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n"
     ]
    }
   ],
   "source": [
    "# pyTorch 현재 텐서의 차원을 보는법 \n",
    "print(\"pyTorch 텐서 차원 info\")\n",
    "print(t_torch.dim()) # rank 즉 차원 \n",
    "print(t_torch.shape) # shape \n",
    "print(t_torch.size()) # shape \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyTorch 텐서 인덱스 접근\n",
      "tensor(0.) tensor(1.) tensor(6.)\n",
      "\n",
      "pyTorch  슬라이싱\n",
      "tensor([2., 3., 4.]) tensor([4., 5.])\n"
     ]
    }
   ],
   "source": [
    "# 1차원 텐서 , 원소는 7개 / 인덱스 접근 과 슬라이싱 실습 \n",
    "print(\"pyTorch 텐서 인덱스 접근\")\n",
    "print(t_torch[0] , t_torch[1], t_torch[-1]) # 인덱스 접근 t[-1] -> 뒤에서 인덱스 접근 즉 ) [0. 1. 2.] 출력은 2.0 (맨 뒤부터 인덱스 접근)\n",
    "\n",
    "print(\"\")\n",
    "print(\"pyTorch  슬라이싱\")\n",
    "# [0. 1. 2. 3. 4. 5. 6.]\n",
    "# start 2 index - end 5 index / start 4 index end -1 (맨뒤 인덱스 첫번째)\n",
    "print(t_torch[2:5] , t_torch[4:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2D with pyTorch test print : \n",
      " tensor([[ 1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.],\n",
      "        [ 7.,  8.,  9.],\n",
      "        [10., 11., 12.]])\n"
     ]
    }
   ],
   "source": [
    "# 2D with PyTorch \n",
    "t_2d_pyTorch = torch.FloatTensor([[1., 2., 3.],\n",
    "                                   [4., 5., 6.],\n",
    "                                   [7., 8., 9.],\n",
    "                                   [10., 11., 12.]\n",
    "                                 ])\n",
    "\n",
    "print(\"2D with pyTorch test print : \\n\", t_2d_pyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyTorch 텐서 차원 info\n",
      "2\n",
      "torch.Size([4, 3])\n",
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "# 2D info \n",
    "print(\"pyTorch 텐서 차원 info\")\n",
    "print(t_2d_pyTorch.dim())\n",
    "print(t_2d_pyTorch.size())\n",
    "print(t_2d_pyTorch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2차원 텐서 인덱스 슬라이싱\n",
      "tensor([ 1.,  4.,  7., 10.])\n",
      "torch.Size([4])\n",
      "\n",
      "tensor([[ 1.,  2.],\n",
      "        [ 4.,  5.],\n",
      "        [ 7.,  8.],\n",
      "        [10., 11.]])\n"
     ]
    }
   ],
   "source": [
    "# 2차원 텐서 슬라이싱 \n",
    "print(\"2차원 텐서 인덱스 슬라이싱\")\n",
    "print(t_2d_pyTorch[:,0])  # 첫번째 차원을 전체 선택한 상황에서 두번째 차원의 첫번째 것만 가져옴\n",
    "print(t_2d_pyTorch[:,0].size()) # 슬라이싱 한 텐서 size 알고 싶은경우 \n",
    "print(\"\")\n",
    "\n",
    "# 만약 맨 첫번째 차원을 선택한 상황에서 두번째 차원에서는 맨 마지막에서 첫번째를 제외하고 다 가져오고 싶은경우 \n",
    "print(t_2d_pyTorch[:, :-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 5.]])\n",
      "\n",
      "m1, m2 size info print code\n",
      "print m1 size =  torch.Size([1, 2])\n",
      "print m2 size =  torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "# 브로드 캐스팅 \n",
    "\"\"\"\n",
    "두 행렬 A, B가 있다고 해봅시다. 행렬의 덧셈과 뺄셈에 대해 알고계신다면, \n",
    "이 덧셈과 뺄셈을 할 때에는 두 행렬 A, B의 크기가 같아야한다는 것을 알고계실겁니다. \n",
    "그리고 두 행렬이 곱셈을 할 때에는 A의 마지막 차원과 B의 첫번째 차원이 일치해야합니다.\n",
    "\n",
    "물론, 이런 규칙들이 있지만 딥 러닝을 하게되면 불가피하게 크기가 다른 행렬 또는 텐서에 대해서 사칙 연산을 수행할 필요가 있는 경우가 생깁니다. \n",
    "이를 위해 파이토치에서는 자동으로 크기를 맞춰서 연산을 수행하게 만드는 브로드캐스팅이라는 기능을 제공합니다.\n",
    "\"\"\"\n",
    "\n",
    "m1 = torch.FloatTensor([[3,3]])\n",
    "m2 = torch.FloatTensor([[2,2]])\n",
    "\n",
    "print(m1 + m2)\n",
    "print(\"\")\n",
    "# m1 m2 크기는 둘다 (1,2)\n",
    "print(\"m1, m2 size info print code\")\n",
    "print(\"print m1 size = \" , m1.size())\n",
    "print(\"print m2 size = \" , m2.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 5.]])\n",
      "\n",
      "m3, m4 size info print code\n",
      "print m3 size =  torch.Size([1, 2])\n",
      "print m4 size =  torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# 브로드 캐스팅 실습 \n",
    "# Vector + scalar 물론 수학적으로는 원래 연산이 안 되는게 맞지만 파이토치에서는 브로드캐스팅 통해 이를 연산합니다.\n",
    "m3 = torch.FloatTensor([[1,2]])\n",
    "m4 = torch.FloatTensor([3]) # [3] -> [3,3]\n",
    "\n",
    "print(m3 + m4)\n",
    "print(\"\")\n",
    "print(\"m3, m4 size info print code\")\n",
    "print(\"print m3 size = \" , m3.size())\n",
    "print(\"print m4 size = \" , m4.size())\n",
    "\n",
    "# pyTorch는 m4 size (1,2) 로 변경해서 연산을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 5.],\n",
      "        [5., 6.]])\n",
      "\n",
      "m5, m6 size info print code\n",
      "print m5 size =  torch.Size([1, 2])\n",
      "print m6 size =  torch.Size([2, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nA텐서와 B텐서가 있는데 사용자는 이 두 텐서의 크기가 같다고 착각하고 덧셈 연산을 수행했다고 가정하면 \\n하지만 실제로 이 두 텐서의 크기는 달랐고 브로드캐스팅이 수행되어 덧셈 연산이 수행되었습니다.\\n만약 두 텐서의 크기가 다르다고 에러를 발생했다면 사용자는 이 연산이 잘못되었음을 알 수 있지만 브로드 캐스팅으로 진행하면 자동으로 변경되다보니\\n사용자는 나중에 원하는 결과가 나오지 않았더라도 어디서 문제가 발생했는지 찾기가 굉장히 어려울 수 있습니다.\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2x1 Vector + 1x2 Vector\n",
    "\n",
    "m5 = torch.FloatTensor([[1,2]])\n",
    "m6 = torch.FloatTensor(([[3], [4]]))\n",
    "print(m5 + m6)\n",
    "print(\"\")\n",
    "print(\"m5, m6 size info print code\")\n",
    "print(\"print m5 size = \" , m5.size())\n",
    "print(\"print m6 size = \" , m6.size())\n",
    "\n",
    "# m5 크기는 (1,2) m6 크기는 (2,1) 이 두 벡터는 원래 수학적으로 덧셈을 수행 불가능 그러나 파이토치는 두 백터의 크기를 (2,2)로 변경하여 덧셈 수행\n",
    "\n",
    "# 브로드 캐스팅 과정 \n",
    "# [1,2]\n",
    "# ===> [[1,2],\n",
    "#       [1,2]]\n",
    "# [3]\n",
    "# [4]\n",
    "# ===> [[3,3], \n",
    "#       [4,4]]\n",
    "\n",
    "# 단 주의사항 \n",
    "# 브로드캐스팅은 편리하지만, 자동으로 실행되는 기능이므로 사용자 입장에서는 굉장히 주의해서 사용해야합니다\n",
    "\"\"\"\n",
    "A텐서와 B텐서가 있는데 사용자는 이 두 텐서의 크기가 같다고 착각하고 덧셈 연산을 수행했다고 가정하면 \n",
    "하지만 실제로 이 두 텐서의 크기는 달랐고 브로드캐스팅이 수행되어 덧셈 연산이 수행되었습니다.\n",
    "만약 두 텐서의 크기가 다르다고 에러를 발생했다면 사용자는 이 연산이 잘못되었음을 알 수 있지만 브로드 캐스팅으로 진행하면 자동으로 변경되다보니\n",
    "사용자는 나중에 원하는 결과가 나오지 않았더라도 어디서 문제가 발생했는지 찾기가 굉장히 어려울 수 있습니다.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6896, 0.9932, 0.9883],\n",
      "        [0.4162, 0.3552, 0.4196]])\n",
      "torch.FloatTensor of size torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Tensor 생성 \n",
    "# Tensor 생성할 때 대표적으로 사용하는 함수 rand, zeros ones \n",
    "\n",
    "# 랜덤 넘버 생성 \n",
    "x = torch.rand(2,3)\n",
    "print(x)\n",
    "print(\"torch.FloatTensor of size\" , x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 0, 4, 1, 3])\n",
      "torch.LongTensor of size torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "# 주어진 범위 내의 정수를 랜덤하게 생성 randperm(범위)\n",
    "rx = torch.torch.randperm(5)\n",
    "print(rx)\n",
    "print(\"torch.LongTensor of size\" , rx.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "# 모든 값이 0인 zeros tensor 를 생성\n",
    "zx = torch.zeros(2,3 , dtype=torch.long) # dtype=torch.long 원하는 타입을 선언가능\n",
    "print(zx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 1인 ones Tensor 생성 \n",
    "ox = torch.ones(2,3)\n",
    "print(ox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.5000, 1.0000, 1.5000, 2.0000, 2.5000])\n"
     ]
    }
   ],
   "source": [
    "# torch.arange(start,end,step=1) -> [start,end) with step\n",
    "# 시작 / 끝 / 단계 을 인자로 사용하여 시작은 이상 끝은 미만의 범위를 가지고 단계 만큼 간격을 두고 Tensor 생성\n",
    "\n",
    "ax = torch.arange(0, 3 , step = 0.5)\n",
    "print(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# 초기화 되지 않은 5x3 행렬 생성 \n",
    "# 초기화되지 않은 행렬이 선언되었지만 사용하기 전에는 명확히 알려진 값을 포함하고 있지는 않습니다.\n",
    "# 즉 초기화되지 않은 행렬이 생성되면 그 시점에 할당된 메모리에 존재하던 값들이 초기값으로 나타납니다.\n",
    "\n",
    "ex = torch.empty(5,3)\n",
    "print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('dp': venv)",
   "language": "python",
   "name": "python37764bitdpvenv3835be669c484e1397b1b7ff62365c4c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
